{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with Together AI Open Source Endpoints\n",
    "\n",
    "This notebook tests the RAG application using Together AI endpoints instead of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up Together API key\n",
    "if \"TOGETHER_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter your Together API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set your custom endpoint from the endpoint_slammer notebook\n",
    "# Replace with your actual endpoint identifier if you have a dedicated one\n",
    "# os.environ[\"TOGETHER_MODEL_ENDPOINT\"] = \"your-username/openai/gpt-oss-20b-identifier\"\n",
    "\n",
    "# Or use the default open source model\n",
    "os.environ[\"TOGETHER_MODEL_ENDPOINT\"] = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory\n",
    "os.environ[\"RAG_DATA_DIR\"] = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the RAG Module\n",
    "\n",
    "Now let's import and test our agentic RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_rag import _get_rag_graph, retrieve_information\n",
    "\n",
    "# Build the RAG graph\n",
    "print(\"Building RAG graph with Together AI endpoints...\")\n",
    "graph = _get_rag_graph()\n",
    "print(\"RAG graph ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single query\n",
    "query = \"How are people using AI in their daily work?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "result = graph.invoke({\"question\": query})\n",
    "print(f\"Response: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple queries\n",
    "test_queries = [\n",
    "    \"What are the main applications of AI mentioned in the document?\",\n",
    "    \"What challenges do people face when using AI?\",\n",
    "    \"How effective is AI for knowledge workers?\",\n",
    "    \"What percentage of people use AI regularly?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    result = graph.invoke({\"question\": query})\n",
    "    print(f\"\\nResponse: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Using the Tool Interface\n",
    "\n",
    "We can also test the RAG system as a tool, which is how it would be used in an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using the tool interface\n",
    "tool_query = \"What insights does the document provide about AI adoption?\"\n",
    "print(f\"Tool Query: {tool_query}\\n\")\n",
    "\n",
    "response = retrieve_information.invoke({\"query\": tool_query})\n",
    "print(f\"Tool Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Query\n",
    "\n",
    "Try your own query below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query\n",
    "custom_query = \"\"  # Enter your question here\n",
    "\n",
    "if custom_query:\n",
    "    result = graph.invoke({\"question\": custom_query})\n",
    "    print(f\"Query: {custom_query}\")\n",
    "    print(f\"\\nResponse: {result['response']}\")\n",
    "else:\n",
    "    print(\"Enter a query in the custom_query variable above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
