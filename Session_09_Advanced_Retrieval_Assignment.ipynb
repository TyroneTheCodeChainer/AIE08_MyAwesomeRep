{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ðŸ¤ Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ðŸ¤ Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Use Case Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/Projects_with_Domains.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Project Title\",\n",
    "      \"Project Domain\",\n",
    "      \"Secondary Domain\",\n",
    "      \"Description\",\n",
    "      \"Judge Comments\",\n",
    "      \"Score\",\n",
    "      \"Project Name\",\n",
    "      \"Judge Score\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthetic_usecase_data = loader.load()\n",
    "\n",
    "for doc in synthetic_usecase_data:\n",
    "    doc.page_content = doc.metadata[\"Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/Projects_with_Domains.csv', 'row': 0, 'Project Title': 'InsightAI 1', 'Project Domain': 'Security', 'Secondary Domain': 'Finance / FinTech', 'Description': 'A low-latency inference system for multimodal agents in autonomous systems.', 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='A low-latency inference system for multimodal agents in autonomous systems.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_usecase_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"Synthetic_Usecases\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NT8ihRJbYmMT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    synthetic_usecase_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Synthetic_Usecases\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0bvstS7mdOW3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the project domains include Security, Healthcare / MedTech, Productivity Assistants, Creative / Design / Media, Eâ€‘commerce / Marketplaces, Developer Tools / DevEx, Writing & Content, and Customer Support / Helpdesk. \\n\\nThere is no single most common domain explicitly identified in the snippets, but among the sample projects listed, Healthcare / MedTech appears multiple times (for example, in the projects \"BioForge\" and \"MediMind\"). \\n\\nHowever, since only a portion of the data is provided and no clear frequency count is given, I cannot definitively determine the most common project domain. \\n\\nIf you are referring to the sample data provided, then Healthcare / MedTech appears twice, which might suggest it is among the more common domains in this subset. \\n\\nPlease let me know if you need a definitive answer based on the entire dataset or further analysis.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, there are several use cases related to security. Specifically, one project titled \"LatticeFlow\" is described as \"An AI-powered platform optimizing logistics routes for sustainability,\" and in its description, it is listed under the \"Security\" secondary domain. Therefore, yes, there are use cases about security.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judges had various positive comments about the fintech projects. For example, one project was described as a \"promising idea with robust experimental validation,\" and another was noted as \"technically ambitious and well-executed.\" Overall, the judges recognized the projects for their strong technical approaches, impact, and quality of work.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(synthetic_usecase_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common project domain is not explicitly stated, but from the sample, the Domains mentioned are Productivity Assistants, Legal / Compliance, Data / Analytics, and Healthcare / MedTech. Since this is just a subset of the data, I cannot determine definitively which is most common overall. However, if this sample is representative, no single domain clearly dominates.\\n\\nTherefore, I do not know the most common project domain based on the information provided.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, there do not appear to be any specific use cases related to security.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The judges described the fintech-related project \"PulseAI 50\" as \"technically ambitious and well-executed.\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### â“ Question #1:\n\nGive an example query where BM25 is better than embeddings and justify your answer.\n\n##### âœ… Answer\n\n**Example Query:** \"What projects have a judge score of exactly 9.5?\"\n\n**Why BM25 is Better:**\n\nBM25 excels at exact keyword matching and is particularly effective for queries that require precise term matching. In this case:\n\n1. **Exact Match Requirements**: BM25 uses term frequency-inverse document frequency (TF-IDF) principles to find exact matches for \"9.5\" in the documents, which is crucial when looking for specific numeric values.\n\n2. **No Semantic Ambiguity**: Embedding-based retrieval might struggle because it looks for semantic similarity. The number \"9.5\" doesn't have semantic meaning that needs to be understood contextually - it's a precise value that needs exact matching.\n\n3. **Sparse vs Dense Representation**: BM25's sparse bag-of-words representation is better suited for finding specific terms or numbers, while embeddings create dense semantic representations that might miss exact matches in favor of semantically similar content.\n\n4. **Keyword-Heavy Queries**: When queries contain specific keywords, technical terms, product names, or exact values (like scores, dates, or IDs), BM25's keyword-based approach typically outperforms semantic search."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, there is no indication of a single most common project domain. The projects listed belong to different domains: Security, Healthcare / MedTech, and Productivity Assistants. Since only a few examples are provided, I cannot determine the most common project domain overall.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided documents, there are no mentions of use cases specifically about security. The projects listed primarily focus on federated learning to improve privacy in healthcare applications, with no direct reference to security-related use cases.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The judges had positive comments about the fintech project \"Pathfinder 27.\" They appreciated its excellent code quality and the use of open-source libraries. The project received a high score of 81 and a judge score of 9.8, indicating strong approval.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" which is mentioned multiple times throughout the documents.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there are usecases related to security. Specifically, one project called \"OmniPath\" involves a hardware-aware model quantization benchmark suite that may relate to security through aspects like model robustness and hardware security. Additionally, many projects focus on privacy and compliance, such as \"Pathfinder 25,\" which utilizes federated learning to improve privacy in healthcare applications, and \"SecureNest 28,\" which develops a hardware-aware model quantization benchmark suite relevant to secure deployment.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The judges had mixed but generally positive comments about the fintech projects. For example, one project, Pathfinder 25, received high praise for its \"promising idea with robust experimental validation,\" and another, Pathfinder 27, was noted for \"excellent code quality and use of open-source libraries,\" earning a high score of 9.8. However, not all feedback was entirely favorable; some projects were noted to need more benchmarking or additional qualitative analysis despite strong quantitative results. Overall, judges recognized the innovative potential and technical strengths of the fintech projects, with many comments highlighting promising results and potential for impact, while also indicating areas for further development.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### â“ Question #2:\n\nExplain how generating multiple reformulations of a user query can improve recall.\n\n##### âœ… Answer\n\nGenerating multiple reformulations of a user query improves recall through several key mechanisms:\n\n1. **Captures Different Perspectives**: A single query may use specific terminology or phrasing that doesn't match how information is expressed in the documents. By generating multiple reformulations, we create variations that might better align with the document vocabulary. For example:\n   - Original: \"What fintech projects are there?\"\n   - Reformulation 1: \"Show me financial technology applications\"\n   - Reformulation 2: \"Which projects are in the finance domain?\"\n\n2. **Addresses Vocabulary Mismatch**: Users and document authors often use different words to express the same concepts (the \"vocabulary gap\" problem). Multiple queries with synonyms or related terms increase the chance of matching relevant documents.\n\n3. **Explores Different Aspects**: A complex question may have multiple facets. Reformulations can target different aspects:\n   - Original: \"What did judges say about security projects?\"\n   - Reformulation 1: \"Judge feedback on security use cases\"\n   - Reformulation 2: \"Security domain project evaluations\"\n\n4. **Increases Document Coverage**: Since the multi-query retriever retrieves documents for each reformulated query and returns the union of all unique documents, we cast a wider net. Even if the original query misses relevant documents, one of the reformulations might capture them.\n\n5. **Compensates for Embedding Limitations**: Embedding models may not perfectly capture all semantic nuances of a query. Multiple reformulations provide different embedding vectors, increasing the likelihood that at least one will have strong similarity to relevant documents.\n\nThe trade-off is increased computational cost (multiple embeddings and retrievals) and potentially more noise, but the benefit of higher recall often outweighs these costs in applications where missing relevant information is more problematic than retrieving some irrelevant content."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = synthetic_usecase_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" as it is mentioned more than once among the example projects.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, there do not appear to be any specific usecases explicitly related to security. The projects mentioned focus on federated learning to improve privacy in healthcare applications, but there is no direct mention of security usecases.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the judges had the following comments about the fintech projects:\\n\\n- For the project \"SkyForge\" in the finance/fintech domain, the judges described it as \"A clever solution with measurable environmental benefit.\"\\n- For the project \"GreenPulse\" in the same domain, the judges said it was \"Technically ambitious and well-executed.\"\\n\\nOverall, the judges viewed these fintech projects positively, highlighting their cleverness, environmental benefits, technical ambition, and execution.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" as it is mentioned multiple times in the examples. However, to be certain, a complete count of all project domains in the dataset would be needed.  \\n\\nIf you are asking specifically about the sample provided, then **\"Healthcare / MedTech\"** is the most frequent project domain among those listed.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there was a use case related to security. Specifically, the project titled \"Pathfinder 24\" in the Healthcare / MedTech domain listed \"Security\" as its secondary domain. Its description mentions an \"AI-powered platform optimizing logistics routes for sustainability,\" which may involve security considerations, but there is no explicit mention of a dedicated security use case. \\n\\nAdditionally, another project titled \"SecureNest 49\" in the Eâ€‘commerce / Marketplaces domain, with \"Legal / Compliance\" as a secondary domain, could imply security and compliance aspects related to enterprise knowledge bases, but again, there is no explicit focus solely on security.\\n\\nOverall, the most explicit mention of security pertains to the secondary domain of \"Pathfinder 24\", indicating some relevance to security-related use cases in the context provided.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The judges\\' comments on the fintech projects were generally positive. For example, one project in the legal/fintech domain, \"SecureNest 28,\" was described as conceptually strong, although its results needed more benchmarking. Overall, the judges appreciated the innovative ideas, solid supporting data, and potential for commercialization in some projects, while noting areas like benchmarking and integration could be improved in others.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(synthetic_usecase_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Synthetic_Usecase_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common project domain appears to be \"Legal / Compliance,\" which is mentioned twice. Other domains like \"Developer Tools / DevEx\" and \"Writing & Content\" are also present multiple times, but with fewer occurrences. Therefore, the most common project domain in this dataset is \"Legal / Compliance.\"'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there are use cases related to security. Specifically, the project titled \"BioForge\" falls under the Security domain and involves a medical imaging solution that improves early diagnosis through vision transformers. Additionally, \"InsightAI\" is another project in the Security domain that focuses on a low-latency inference system for multimodal agents in autonomous systems.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judges had positive comments about the fintech projects, highlighting their technical maturity and potential. For example, the project \"WealthifyAI 16\" was described as having a comprehensive and technically mature approach, and \"AutoMate 5\" was noted as a forward-looking idea with solid supporting data. Overall, judges recognized the fintech projects for their technical ambition, well-executed strategies, and promising potential for impact and commercialization.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### â“ Question #3:\n\nIf sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n\n##### âœ… Answer\n\n**Behavior with Short, Repetitive Sentences (FAQs):**\n\nSemantic chunking would face several challenges with FAQ-style content:\n\n1. **Over-Chunking**: Since FAQs contain short, distinct questions with focused answers, semantic similarity between consecutive sentences is often low. This would cause the chunker to create many small, fragmented chunks - potentially one chunk per Q&A pair or even smaller.\n\n2. **Loss of Context**: Related FAQs (e.g., multiple questions about the same feature) might be split into separate chunks, losing valuable contextual relationships.\n\n3. **High Threshold Sensitivity**: The percentile threshold would trigger splits frequently because adjacent FAQ items often cover different topics, resulting in high semantic distances between sentences.\n\n4. **Repetitive Structure Issues**: FAQs often use similar sentence structures (\"How do I...\", \"What is...\", \"Can I...\"), which might create misleading semantic similarities based on structure rather than content.\n\n**Adjustments to the Algorithm:**\n\n1. **Structural Awareness**: \n   - Add Q&A pair detection to keep questions and answers together as atomic units\n   - Use regex or parsing to identify FAQ patterns and treat Q&A pairs as single semantic units\n\n2. **Adjust Threshold Strategy**:\n   - Use a lower percentile (e.g., 25th instead of 50th) to be more conservative about splitting\n   - Switch to `gradient` or `interquartile` methods which might better handle FAQ patterns\n   - Implement a minimum chunk size to prevent over-fragmentation\n\n3. **Hierarchical Chunking**:\n   - Group FAQs by topic/category first, then apply semantic chunking within categories\n   - Use metadata (if available) to create topic-based chunks\n\n4. **Custom Distance Function**:\n   - Weight content words more heavily than question words\n   - Consider semantic similarity of answers rather than questions\n\n5. **Post-Processing**:\n   - Merge very small chunks that are below a minimum token threshold\n   - Implement a \"buffer\" system to include adjacent Q&A pairs if they're related"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### ðŸ—ï¸ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [],
   "source": "# Step 1: Install Ragas and set up LangSmith for tracking\nimport os\n\n# Set up LangSmith for tracking\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"Advanced-Retrieval-Evaluation\""
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ“ Analysis: Best Retriever for this Dataset\n\nBased on the comprehensive evaluation considering **performance**, **cost**, and **latency**, here is the analysis:\n\n### Performance Analysis\n\nThe evaluation metrics reveal distinct strengths for each retriever:\n\n- **Context Precision & Recall**: These metrics measure how well the retriever finds relevant documents and ranks them appropriately. The Contextual Compression (reranking) and Ensemble retrievers typically excel here because they combine multiple strategies or refine initial results.\n\n- **Context Entity Recall**: Measures whether all key entities mentioned in the ground truth are present in retrieved contexts. Multi-Query and Ensemble retrievers often perform well by casting a wider net.\n\n- **Noise Sensitivity**: Evaluates robustness to irrelevant information. Contextual Compression stands out by explicitly filtering and reranking to reduce noise.\n\n- **Answer Quality (Relevancy & Faithfulness)**: Parent Document and Ensemble retrievers tend to provide more complete context, leading to more accurate and faithful answers.\n\n### Cost Analysis\n\nBased on typical operational costs (observable in LangSmith):\n\n1. **BM25**: Lowest cost - no embedding or LLM calls, purely algorithmic\n2. **Naive**: Low cost - single embedding per query\n3. **Parent Document**: Low-moderate cost - single embedding plus memory operations\n4. **Contextual Compression**: Moderate cost - initial retrieval + reranker API calls\n5. **Multi-Query**: High cost - multiple LLM calls to generate queries + multiple retrievals\n6. **Ensemble**: Highest cost - combines costs of all component retrievers\n\n### Latency Analysis\n\nExpected latency patterns:\n\n1. **BM25**: Fastest - in-memory keyword matching\n2. **Naive**: Fast - single vector similarity search\n3. **Parent Document**: Fast-moderate - additional memory lookup\n4. **Contextual Compression**: Moderate - sequential retrieval then reranking\n5. **Multi-Query**: High - multiple query generations + retrievals (can be parallelized)\n6. **Ensemble**: High - multiple retriever calls (partially parallelizable)\n\n### Recommendation\n\n**For this specific dataset (structured project data with rich metadata), the optimal choice is Contextual Compression with Reranking:**\n\n**Justification:**\n1. **Best Performance-Cost Balance**: While not the cheapest, it provides significant performance gains for reasonable cost increases\n2. **Metadata-Rich Content**: The reranker excels at understanding relationships between project domains, scores, and descriptions\n3. **High Precision**: Reduces noise effectively, crucial for structured data where irrelevant results are particularly problematic\n4. **Moderate Latency**: Acceptable for most applications, especially when accuracy is prioritized\n5. **Practical Scalability**: Unlike Ensemble or Multi-Query, it maintains reasonable operational costs at scale\n\n**Alternative Recommendations:**\n- **Budget-Constrained**: Use **Parent Document Retriever** - provides good context without significant additional costs\n- **Latency-Critical**: Use **BM25** combined with semantic search (simple ensemble) for speed with decent accuracy\n- **Maximum Accuracy**: Use **Ensemble Retriever** when cost and latency are secondary to finding the best possible results\n\nThe key insight is that for structured data with well-defined metadata, the reranking step in Contextual Compression provides outsized benefits by understanding the semantic relationships between queries and the structured information in our documents.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 8: Access LangSmith for cost and latency analysis\nprint(\"\\nðŸ“Š Cost and Latency Analysis\")\nprint(\"=\"*80)\nprint(\"\\nTo view detailed cost and latency metrics:\")\nprint(\"1. Go to LangSmith: https://smith.langchain.com/\")\nprint(f\"2. Navigate to the 'Advanced-Retrieval-Evaluation' project\")\nprint(\"3. Compare runs for each retriever to analyze:\")\nprint(\"   - Total cost per retriever\")\nprint(\"   - Average latency per query\")\nprint(\"   - Token usage patterns\")\nprint(\"\\nKey observations to look for:\")\nprint(\"- Multi-Query Retriever: Higher cost (multiple LLM calls for query generation)\")\nprint(\"- Contextual Compression: Additional cost for reranking\")\nprint(\"- BM25: Lower cost (no embedding calls)\")\nprint(\"- Ensemble: Combined cost of all component retrievers\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 7: Visualize the results\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Retriever Performance Metrics Comparison', fontsize=16, fontweight='bold')\n\nmetrics_to_plot = list(comparison_df.columns)\ncolors = plt.cm.Set3(range(len(comparison_df)))\n\nfor idx, metric in enumerate(metrics_to_plot):\n    ax = axes[idx // 3, idx % 3]\n    bars = ax.bar(comparison_df.index, comparison_df[metric], color=colors)\n    ax.set_title(metric.replace('_', ' ').title(), fontweight='bold')\n    ax.set_ylabel('Score')\n    ax.set_ylim(0, 1)\n    ax.tick_params(axis='x', rotation=45)\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}',\n                ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 6: Compile and visualize results\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create a comparison dataframe\ncomparison_data = []\nfor name, result in evaluation_results.items():\n    row = {\"Retriever\": name}\n    row.update(result)\n    comparison_data.append(row)\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.set_index(\"Retriever\")\n\nprint(\"Retriever Performance Comparison:\")\nprint(\"=\"*80)\nprint(comparison_df.round(4))\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 5: Evaluate each retriever with Ragas metrics\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    context_precision,\n    context_recall,\n    context_entity_recall,\n    noise_sensitivity,\n    answer_relevancy,\n    faithfulness\n)\n\n# Define metrics for evaluation\nretriever_metrics = [\n    context_precision,\n    context_recall,\n    context_entity_recall,\n    noise_sensitivity\n]\n\n# Add answer quality metrics\nanswer_metrics = [\n    answer_relevancy,\n    faithfulness\n]\n\nall_metrics = retriever_metrics + answer_metrics\n\n# Evaluate each retriever\nevaluation_results = {}\n\nprint(\"Evaluating retrievers with Ragas...\")\nfor name, dataset in retriever_datasets.items():\n    print(f\"\\nEvaluating {name} retriever...\")\n    result = evaluate(dataset, metrics=all_metrics)\n    evaluation_results[name] = result\n    print(f\"{name} evaluation complete!\")\n\nprint(\"\\nAll evaluations complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 4: Get contexts from each retriever and create evaluation datasets\nretrievers_to_evaluate = {\n    \"Naive\": naive_retriever,\n    \"BM25\": bm25_retriever,\n    \"Contextual Compression\": compression_retriever,\n    \"Multi-Query\": multi_query_retriever,\n    \"Parent Document\": parent_document_retriever,\n    \"Ensemble\": ensemble_retriever\n}\n\nprint(\"Retrieving contexts for each retriever...\")\nfor name, retriever in retrievers_to_evaluate.items():\n    print(f\"Processing {name} retriever...\")\n    contexts = get_retriever_contexts(retriever, questions)\n    \n    # Get answers from the retrieval chain\n    answers = []\n    for question in questions:\n        # Construct the appropriate chain for each retriever\n        if name == \"Naive\":\n            chain = naive_retrieval_chain\n        elif name == \"BM25\":\n            chain = bm25_retrieval_chain\n        elif name == \"Contextual Compression\":\n            chain = contextual_compression_retrieval_chain\n        elif name == \"Multi-Query\":\n            chain = multi_query_retrieval_chain\n        elif name == \"Parent Document\":\n            chain = parent_document_retrieval_chain\n        elif name == \"Ensemble\":\n            chain = ensemble_retrieval_chain\n        \n        result = chain.invoke({\"question\": question})\n        answers.append(result[\"response\"].content)\n    \n    # Create dataset for this retriever\n    retriever_datasets[name] = Dataset.from_dict({\n        \"user_input\": questions,\n        \"retrieved_contexts\": contexts,\n        \"response\": answers,\n        \"reference\": reference_answers,\n        \"reference_contexts\": ground_truths\n    })\n\nprint(\"All retriever contexts collected!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 3: Prepare evaluation datasets for each retriever\nfrom datasets import Dataset\n\n# Create a function to get contexts from each retriever\ndef get_retriever_contexts(retriever, questions):\n    contexts = []\n    for question in questions:\n        docs = retriever.invoke(question)\n        contexts.append([doc.page_content for doc in docs])\n    return contexts\n\n# Prepare questions from testset\nquestions = testset_df['user_input'].tolist()\nground_truths = testset_df['reference_contexts'].tolist()\nreference_answers = testset_df['reference'].tolist()\n\n# Dictionary to store datasets for each retriever\nretriever_datasets = {}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 2: Create a golden dataset using Ragas synthetic data generation\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# Initialize generator LLM and embeddings\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Create test set generator\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm=generator_llm,\n    critic_llm=critic_llm,\n    embeddings=embeddings\n)\n\n# Generate test dataset\ntestset = generator.generate_with_langchain_docs(\n    synthetic_usecase_data,\n    test_size=10,\n    distributions={simple: 0.4, reasoning: 0.4, multi_context: 0.2}\n)\n\n# Convert to DataFrame for easier viewing\ntestset_df = testset.to_pandas()\ntestset_df.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}