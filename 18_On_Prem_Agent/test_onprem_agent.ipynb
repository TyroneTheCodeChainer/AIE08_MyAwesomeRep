{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Premises RAG Agent Testing\n",
    "\n",
    "This notebook tests the fully on-premises RAG agent that uses:\n",
    "- **Ollama** for LLM and embeddings (no cloud APIs!)\n",
    "- **Qdrant** for vector storage (running locally via Docker)\n",
    "- **LangGraph** for agent orchestration\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Ollama** must be running with models pulled:\n",
    "   ```bash\n",
    "   ollama pull deepseek-r1:8b\n",
    "   ollama pull mxbai-embed-large\n",
    "   ```\n",
    "\n",
    "2. **Qdrant** must be running via Docker:\n",
    "   ```bash\n",
    "   docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from onprem_rag_agent import (\n",
    "    create_onprem_agent,\n",
    "    chat_with_agent,\n",
    "    get_or_create_vectorstore,\n",
    "    retrieve_dnd_information\n",
    ")\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Qdrant (Optional)\n",
    "\n",
    "By default, it connects to `127.0.0.1:6334`. Change if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set custom Qdrant URL\n",
    "# os.environ[\"QDRANT_URL\"] = \"127.0.0.1:6334\"\n",
    "# os.environ[\"QDRANT_COLLECTION\"] = \"DnD_Documents\"\n",
    "\n",
    "print(\"Qdrant URL:\", os.environ.get(\"QDRANT_URL\", \"127.0.0.1:6334\"))\n",
    "print(\"Collection:\", os.environ.get(\"QDRANT_COLLECTION\", \"DnD_Documents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Vector Store\n",
    "\n",
    "This will either connect to an existing collection or create a new one.\n",
    "\n",
    "**Note:** Creating a new collection will take 10-15 minutes as it needs to embed ~5800 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up vector store...\")\n",
    "print(\"This may take a while if creating a new collection...\\n\")\n",
    "\n",
    "vectorstore = get_or_create_vectorstore(\n",
    "    qdrant_url=\"127.0.0.1:6334\",\n",
    "    collection_name=\"DnD_Documents\",\n",
    "    recreate=False  # Set to True to force recreation\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Vector store ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test RAG Tool Directly\n",
    "\n",
    "Let's test the RAG retrieval tool before using the full agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG tool directly\n",
    "query = \"What feats improve strength?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "result = retrieve_dnd_information.invoke({\"query\": query})\n",
    "print(f\"Result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the Agent\n",
    "\n",
    "Now let's create the full agent with tool use capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating on-premises agent...\\n\")\n",
    "\n",
    "agent = create_onprem_agent(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Agent created and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some feats that improve a character's strength?\"\n",
    "print(f\"User: {query}\\n\")\n",
    "\n",
    "response = chat_with_agent(agent, query)\n",
    "print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"Tell me about feats related to magic or spellcasting\",\n",
    "    \"What feats are good for a rogue character?\",\n",
    "    \"What feats help with armor class?\",\n",
    "    \"Are there feats that improve initiative?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    response = chat_with_agent(agent, query)\n",
    "    print(f\"\\nAgent: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Chat\n",
    "\n",
    "Try your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query\n",
    "custom_query = \"\"  # Enter your question here\n",
    "\n",
    "if custom_query:\n",
    "    print(f\"User: {custom_query}\\n\")\n",
    "    response = chat_with_agent(agent, custom_query)\n",
    "    print(f\"Agent: {response}\")\n",
    "else:\n",
    "    print(\"Enter a query in the custom_query variable above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Inspect Agent Execution\n",
    "\n",
    "Let's see the full execution trace to understand what the agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with full output to see tool calls\n",
    "query = \"What feats help with stealth?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL EXECUTION TRACE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    print(f\"\\nMessage {i+1}:\")\n",
    "    print(f\"Type: {type(message).__name__}\")\n",
    "    print(f\"Content: {message.content[:200]}...\") if len(message.content) > 200 else print(f\"Content: {message.content}\")\n",
    "\n",
    "    if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "        print(f\"Tool Calls: {message.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully tested a fully on-premises RAG agent that:\n",
    "- ✅ Runs completely locally (no cloud APIs)\n",
    "- ✅ Uses Ollama for LLM and embeddings\n",
    "- ✅ Uses Qdrant for vector storage\n",
    "- ✅ Uses LangGraph for agent orchestration\n",
    "- ✅ Can retrieve and reason over D&D knowledge\n",
    "\n",
    "This demonstrates how to build production-ready AI applications without depending on external APIs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
