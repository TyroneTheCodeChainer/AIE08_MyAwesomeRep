{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 2: Embeddings and RAG Assignment\n",
        "\n",
        "## Overview\n",
        "This notebook implements a Retrieval Augmented Generation (RAG) system using Python, OpenAI embeddings, and vector similarity search.\n",
        "\n",
        "## Tasks\n",
        "1. **Imports and Utilities** - Set up required libraries and helper functions\n",
        "2. **Documents** - Load and process text documents\n",
        "3. **Embeddings and Vectors** - Generate embeddings and create vector database\n",
        "4. **Prompts** - Create effective prompts for the RAG system\n",
        "5. **Retrieval Augmented Generation** - Implement the complete RAG pipeline\n",
        "\n",
        "## Activity #1: Augment RAG\n",
        "Enhance the RAG system with additional features like PDF support, metadata, or different embedding models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Imports and Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# OpenAI and embeddings\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Vector operations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "\n",
        "# Document processing\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up OpenAI client\n",
        "client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "print(\"✅ Imports completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: What is the purpose of using async/await in the OpenAI client setup?\n",
        "\n",
        "**Answer:** Async/await allows for non-blocking operations when making API calls to OpenAI. This means that when generating embeddings for multiple documents, the code can process multiple requests concurrently rather than waiting for each one to complete sequentially. This significantly improves performance and reduces total processing time, especially when dealing with large document collections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Document:\n",
        "    \"\"\"Represents a document with content and metadata.\"\"\"\n",
        "    content: str\n",
        "    source: str\n",
        "    page_number: Optional[int] = None\n",
        "    file_type: str = \"text\"\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        # Clean and normalize content\n",
        "        self.content = self.content.strip()\n",
        "        if len(self.content) < 10:\n",
        "            raise ValueError(\"Document content too short\")\n",
        "\n",
        "def load_text_file(file_path: str) -> Document:\n",
        "    \"\"\"Load a text file and return a Document object.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return Document(\n",
        "            content=content,\n",
        "            source=file_path,\n",
        "            file_type=\"text\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_pdf_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"Load a PDF file and return a list of Document objects (one per page).\"\"\"\n",
        "    documents = []\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                content = page.extract_text().strip()\n",
        "                if content and len(content) > 10:\n",
        "                    documents.append(Document(\n",
        "                        content=content,\n",
        "                        source=file_path,\n",
        "                        page_number=page_num,\n",
        "                        file_type=\"pdf\"\n",
        "                    ))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF {file_path}: {e}\")\n",
        "    return documents\n",
        "\n",
        "print(\"✅ Document classes and loading functions created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Why do we use a dataclass for the Document class instead of a regular class?\n",
        "\n",
        "**Answer:** Dataclasses provide several advantages: 1) Automatic generation of `__init__`, `__repr__`, and `__eq__` methods, reducing boilerplate code; 2) Type hints are enforced and provide better IDE support; 3) Cleaner, more readable code structure; 4) Built-in support for immutability with `frozen=True` if needed; 5) Better integration with serialization libraries. For a simple data structure like Document, dataclasses offer the perfect balance of functionality and simplicity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Embeddings and Vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def generate_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
        "    \"\"\"Generate embedding for a single text using OpenAI API.\"\"\"\n",
        "    try:\n",
        "        response = await client.embeddings.create(\n",
        "            model=model,\n",
        "            input=text\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "async def generate_embeddings_batch(documents: List[Document], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
        "    \"\"\"Generate embeddings for multiple documents concurrently.\"\"\"\n",
        "    tasks = [generate_embedding(doc.content, model) for doc in documents]\n",
        "    embeddings = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    \n",
        "    # Filter out failed embeddings\n",
        "    valid_embeddings = [emb for emb in embeddings if isinstance(emb, list)]\n",
        "    print(f\"Generated {len(valid_embeddings)} embeddings out of {len(documents)} documents\")\n",
        "    return valid_embeddings\n",
        "\n",
        "class VectorDatabase:\n",
        "    \"\"\"Simple in-memory vector database for storing and searching embeddings.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.documents: List[Document] = []\n",
        "        self.embeddings: List[List[float]] = []\n",
        "        self.metadata: List[Dict[str, Any]] = []\n",
        "    \n",
        "    def add_documents(self, documents: List[Document], embeddings: List[List[float]]):\n",
        "        \"\"\"Add documents and their embeddings to the database.\"\"\"\n",
        "        self.documents.extend(documents)\n",
        "        self.embeddings.extend(embeddings)\n",
        "        \n",
        "        # Create metadata for each document\n",
        "        for doc in documents:\n",
        "            self.metadata.append({\n",
        "                \"source\": doc.source,\n",
        "                \"page_number\": doc.page_number,\n",
        "                \"file_type\": doc.file_type,\n",
        "                \"content_length\": len(doc.content)\n",
        "            })\n",
        "    \n",
        "    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for similar documents using cosine similarity.\"\"\"\n",
        "        if not self.embeddings:\n",
        "            return []\n",
        "        \n",
        "        # Calculate cosine similarities\n",
        "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
        "        \n",
        "        # Get top-k results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                \"document\": self.documents[idx],\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"metadata\": self.metadata[idx]\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "\n",
        "print(\"✅ Embedding functions and VectorDatabase class created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: What are the advantages of using cosine similarity for document retrieval?\n",
        "\n",
        "**Answer:** Cosine similarity is ideal for document retrieval because: 1) It measures the angle between vectors rather than magnitude, making it robust to document length differences; 2) It's normalized between -1 and 1, providing consistent similarity scores; 3) It's computationally efficient for high-dimensional vectors; 4) It works well with text embeddings where the direction (semantic meaning) matters more than the magnitude; 5) It's less sensitive to document size variations, so short and long documents can be compared fairly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_prompt(query: str, context_documents: List[Document], system_prompt: str = None) -> str:\n",
        "    \"\"\"Create a comprehensive prompt for RAG with context and query.\"\"\"\n",
        "    \n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"\"\"You are a helpful AI assistant that answers questions based on the provided context. \n",
        "Use only the information from the context documents to answer questions. If the context doesn't contain \n",
        "enough information to answer the question, say so clearly. Always cite the source of your information \n",
        "when possible.\"\"\"\n",
        "    \n",
        "    # Build context section\n",
        "    context_sections = []\n",
        "    for i, doc in enumerate(context_documents, 1):\n",
        "        source_info = f\"Source: {doc.source}\"\n",
        "        if doc.page_number:\n",
        "            source_info += f\", Page {doc.page_number}\"\n",
        "        if doc.file_type:\n",
        "            source_info += f\" (Type: {doc.file_type})\"\n",
        "        \n",
        "        context_sections.append(f\"\"\"\n",
        "--- Context Document {i} ---\n",
        "{source_info}\n",
        "Content: {doc.content[:1000]}{'...' if len(doc.content) > 1000 else ''}\n",
        "\"\"\")\n",
        "    \n",
        "    context = \"\\n\".join(context_sections)\n",
        "    \n",
        "    # Create the full prompt\n",
        "    prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "Context Documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "print(\"✅ Prompt creation functions implemented!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Why is it important to include source information in RAG prompts?\n",
        "\n",
        "**Answer:** Including source information in RAG prompts is crucial for several reasons: 1) **Transparency** - Users can verify the information and understand where it came from; 2) **Traceability** - Enables fact-checking and validation of claims; 3) **Trust** - Users are more likely to trust responses when they know the source; 4) **Context** - Source metadata (like page numbers, file types) provides additional context that can improve answer quality; 5) **Debugging** - Helps identify which documents are most relevant for specific queries; 6) **Professionalism** - Makes the system more suitable for production use where accountability matters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Retrieval Augmented Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system that combines retrieval and generation.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_model: str = \"text-embedding-3-small\", llm_model: str = \"gpt-4o-mini\"):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm_model = llm_model\n",
        "        self.vector_db = VectorDatabase()\n",
        "        self.client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "    \n",
        "    async def add_documents(self, documents: List[Document]):\n",
        "        \"\"\"Add documents to the RAG system.\"\"\"\n",
        "        print(f\"Generating embeddings for {len(documents)} documents...\")\n",
        "        embeddings = await generate_embeddings_batch(documents, self.embedding_model)\n",
        "        \n",
        "        # Filter documents to match successful embeddings\n",
        "        valid_docs = [doc for i, doc in enumerate(documents) if i < len(embeddings) and embeddings[i] is not None]\n",
        "        valid_embeddings = [emb for emb in embeddings if emb is not None]\n",
        "        \n",
        "        self.vector_db.add_documents(valid_docs, valid_embeddings)\n",
        "        print(f\"Added {len(valid_docs)} documents to the vector database\")\n",
        "    \n",
        "    async def query(self, question: str, top_k: int = 3, include_sources: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system with a question.\"\"\"\n",
        "        # Generate embedding for the question\n",
        "        query_embedding = await generate_embedding(question, self.embedding_model)\n",
        "        if not query_embedding:\n",
        "            return {\"error\": \"Failed to generate query embedding\"}\n",
        "        \n",
        "        # Retrieve relevant documents\n",
        "        search_results = self.vector_db.search(query_embedding, top_k=top_k)\n",
        "        \n",
        "        if not search_results:\n",
        "            return {\"answer\": \"No relevant documents found.\", \"sources\": []}\n",
        "        \n",
        "        # Extract documents and create context\n",
        "        context_documents = [result[\"document\"] for result in search_results]\n",
        "        \n",
        "        # Create RAG prompt\n",
        "        prompt = create_rag_prompt(question, context_documents)\n",
        "        \n",
        "        # Generate answer using LLM\n",
        "        try:\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.llm_model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=500,\n",
        "                temperature=0.1\n",
        "            )\n",
        "            \n",
        "            answer = response.choices[0].message.content\n",
        "            \n",
        "            result = {\"answer\": answer}\n",
        "            \n",
        "            if include_sources:\n",
        "                result[\"sources\"] = [\n",
        "                    {\n",
        "                        \"source\": doc.source,\n",
        "                        \"page_number\": doc.page_number,\n",
        "                        \"file_type\": doc.file_type,\n",
        "                        \"similarity\": search_results[i][\"similarity\"]\n",
        "                    }\n",
        "                    for i, doc in enumerate(context_documents)\n",
        "                ]\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"Failed to generate answer: {str(e)}\"}\n",
        "\n",
        "print(\"✅ Complete RAG system implemented!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
